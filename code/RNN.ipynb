{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.io as spio\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readMat(dataPath):\n",
    "    \n",
    "    \n",
    "    #readMat: Reads the .mat for mad river data files placed at data path\n",
    "    #INPUT\n",
    "    #dataPath == relative location of the data path to the code file, or absolute\n",
    "    #\n",
    "    #OUTPUT\n",
    "    #still being worked on\n",
    "    #eventToClass== np array of all classes belonging to all 616events. value at 0 index is class for 0th event.\n",
    "    #sedimentData_events == list containing array for sediment data for each event\n",
    "    #\n",
    "    #\n",
    "    #events == list containing 2-D array for each event. 1st column is sediment and 2nd is stream flow. Ideally we will be using this\n",
    "    #maxEventLen == Longest event in terms of timesteps\n",
    "    \n",
    "    #sample call: eventToClass, myEvents, maxEventLen,streamFlow_Data,sedimentData_events = readMat('..\\data\\')\n",
    "    \n",
    "    #Programmer: Ali Javed\n",
    "    #Date last modified: 27 Feb 2018\n",
    "    #modified by: Ali Javed\n",
    "    #Comments: Initial version.\n",
    "    \n",
    "    \n",
    "    ##############################################################################################################\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    classMat = spio.loadmat(dataPath + 'allMadSitesStormHystClassKey.mat', squeeze_me=True)\n",
    "    dataMat = spio.loadmat(dataPath + 'allMadSitesEventTimeSeries.mat', squeeze_me=True)\n",
    "    \n",
    "    eventToClass = classMat['stormHystClass'][:,3] #index 3 refers to class of 3rd event. Event number start from 0\n",
    "    eventToClass = eventToClass.astype(int) # we do not need float classes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #gather 626 events\n",
    "    events = []\n",
    "    sedimentData_events = []\n",
    "    streamFlowData_events = []\n",
    "    counter = 0\n",
    "    maxEventLen = -1 #need this as fixed size input to keras RNN\n",
    "    \n",
    "    streamFlow = 1\n",
    "    suspendedSedimentConcentration = 2\n",
    "    \n",
    "    for event in range(0,len(dataMat['dataTSOut'])):\n",
    "    \n",
    "    \n",
    "        #not reading datetime and rainfall data for now\n",
    "        #event_dataTime = np.zeros((len(dataMat['dataTSOut'][event][streamFlow]))) #can not extract datetime so setting it to one, for out purpose it does not matter anyways\n",
    "        #event_rainFall = np.zeros((len(dataMat['dataTSOut'][event][streamFlow])))\n",
    "                             \n",
    "        event_streamflow = dataMat['dataTSOut'][event][streamFlow]\n",
    "        event_suspendedSedimentConcentration = dataMat['dataTSOut'][event][suspendedSedimentConcentration]\n",
    "    \n",
    "        \n",
    "        eventArray = np.column_stack((event_streamflow,event_suspendedSedimentConcentration))\n",
    "        \n",
    "        \n",
    "        events.append(eventArray)\n",
    "        sedimentData_events.append(event_streamflow)\n",
    "        streamFlowData_events.append(event_suspendedSedimentConcentration)\n",
    "    \n",
    "        if len(event_streamflow)> maxEventLen:\n",
    "            maxEventLen = len(event_streamflow)\n",
    "    \n",
    "    \n",
    "        \n",
    "        ##############################################################################################################\n",
    "        #for classification based only on rain and sediment... i can not figure out how to give 2d input to RNN\n",
    "        \n",
    "        \n",
    "        #classVector = np.repeat(eventToClass[event], len(event_streamflow))\n",
    "        #print(np.shape(classVector))\n",
    "        #print(np.shape(suspendedSedimentConcentration))\n",
    "        #streamFlow_Data = np.column_stack((event_streamflow,classVector))\n",
    "        #suspendedSedimentConcentration_Data = np.column_stack((event_suspendedSedimentConcentration,classVector))\n",
    "        \n",
    "    return eventToClass, events, maxEventLen, streamFlowData_events, sedimentData_events\n",
    "    \n",
    "    \n",
    " ##############################################################################################################\n",
    "       \n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    " \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataPath = '../data/'\n",
    "\n",
    "#eventToClass as an array of len(events) with each index telling the class of event\n",
    "#myEvents is a list containing 2-d arrays for all events. 0 column is the stream flow, 1 column is sediment concentration\n",
    "eventToClass, myEvents, maxEventLen,streamFlow_Data,sedimentData_events = readMat(dataPath)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#append two arrays into single for each even, apparantely it doesnt matter what order data is passed to NN for 2 d events, as long as format is consitent\n",
    "#i.e first 20 rows for sediment rate, next 20 for rain\n",
    "rain_sediment = []\n",
    "for i in range(0,len(sedimentData_events)):\n",
    "    t1 = sedimentData_events[i]\n",
    "    t2 = streamFlow_Data[i]\n",
    "    \n",
    "    rain_sediment_event = np.append(t1,t2)\n",
    "    rain_sediment.append(rain_sediment_event)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train test split startified not being done for now\n",
    "\n",
    "#preprocess data to required format, padding to make all sequence data same lenght\n",
    "#to use sediment rate onle, replace with sedimentData_events, for stream for data only streamFlow_Data, for both sediment and stream flow rain_sediment \n",
    "X_data = sequence.pad_sequences(rain_sediment,dtype='float')\n",
    "#create one hot representation for class values\n",
    "y_data = to_categorical(eventToClass, num_classes=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Test/Train Split\n",
    "\n",
    "sampler = StratifiedKFold(n_splits=4, shuffle=False, random_state=None)\n",
    "#sampler.split(x= X_data,y=eventToClass)\n",
    "\n",
    "for train_index, test_index in sampler.split(X_data, eventToClass):\n",
    "    X_train, X_test = X_data[train_index], X_data[test_index]\n",
    "    y_train, y_test = y_data[train_index], y_data[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "#DECLARE PARAMETERS FOR NN\n",
    "#these parameters are the architecture of the RNN. Still have to do more on this\n",
    "embedding_vecor_length = 32  #each event is represented using a 32 length vector. \n",
    "epochs = 10\n",
    "batchSize = len(X_train)  #use all data.\n",
    "maxEventLenPram = maxEventLen *2 # if we are passing both rain and sediment in 1 d use multiply maxevent lenght by 2, this is input size if trying only rain or sediment, use maxeventlen\n",
    "m = np.amax(X_train)+1 #what is the maximum data value\n",
    "m = round(m)\n",
    "len_input = int(m)\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 313, 32)           22400     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                1616      \n",
      "=================================================================\n",
      "Total params: 77,216\n",
      "Trainable params: 77,216\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "475/475 [==============================] - 20s 42ms/step - loss: 2.7236 - acc: 0.1874\n",
      "Epoch 2/3\n",
      "475/475 [==============================] - 19s 40ms/step - loss: 2.4640 - acc: 0.2000\n",
      "Epoch 3/3\n",
      "475/475 [==============================] - 19s 40ms/step - loss: 2.4394 - acc: 0.2000\n",
      "151/151 [==============================] - 1s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.4016843742092715, 0.20529801373844905]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###############################################################################\n",
    "#CREATE SIMPLE RNN\n",
    "\n",
    "\n",
    "# create the model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(len_input, embedding_vecor_length, input_length=maxEventLenPram))\n",
    "#add the recurrent LSTM layer of 100 nodes\n",
    "model.add(LSTM(100))\n",
    "\n",
    "#out put layer with 16 nodes, one hot representation\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "#test and train both on same data because did not split for now.\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batchSize)\n",
    "Loss,Acc =model.evaluate(x = X_test, y = y_test)\n",
    "\n",
    "\n",
    "print(\"RNN accuracy is \",Acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 5,\n",
       "         2: 2,\n",
       "         3: 3,\n",
       "         4: 8,\n",
       "         5: 27,\n",
       "         6: 14,\n",
       "         7: 8,\n",
       "         8: 14,\n",
       "         9: 2,\n",
       "         10: 6,\n",
       "         11: 6,\n",
       "         12: 11,\n",
       "         13: 8,\n",
       "         14: 1,\n",
       "         15: 11})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 19,\n",
       "         2: 10,\n",
       "         3: 5,\n",
       "         4: 39,\n",
       "         5: 97,\n",
       "         6: 68,\n",
       "         7: 48,\n",
       "         8: 78,\n",
       "         9: 10,\n",
       "         10: 17,\n",
       "         11: 18,\n",
       "         12: 40,\n",
       "         13: 25,\n",
       "         14: 7,\n",
       "         15: 19})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
